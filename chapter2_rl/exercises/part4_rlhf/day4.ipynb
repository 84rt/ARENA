{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable, Union\n",
    "\n",
    "import os\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "# !pip install eindex\n",
    "import eindex\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "# !pip install transformer_lens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter2_rl\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part4_rlhf\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part4_rlhf.tests as tests\n",
    "import part4_rlhf.solutions as solutions\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "LOW_GPU_MEM = False\n",
    "BASE_MODEL = \"gpt2-small\" if LOW_GPU_MEM else \"gpt2-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c0ec652b4f431a82e8a8635840127c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5e1d646df04bc4b2e9f84ce0360bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4a23e99e39463a8a182154990e76f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa8fce802714697930b15d571f5ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bdac553e6c4c05bf4de32883c5c5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6220f9ffebc44fa58d8e52c1a8b0c8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31be8d020328414391954f125192345b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bart/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n",
      "All tests for `TransformerWithValueHead` passed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "First define self.base_model and self.value_head in your init step (reminder that you should use HookedTransformer.from_pretrained to load in a pretrained model). Then rewrite the forward method so that it outputs both the logits from a forward pass and the output of the value head.\n",
    "\n",
    "The easiest and most direct way to get the output of the value head would be to add a hook to the residual stream before the unembedding matrix, which computes the output of the value head and stores it externally (or as a class attribute). You can review the material from section 1.2 if you don't remember how to use hooks, and you can refer to the diagram on the reference page for how to get the correct hook name.\n",
    "\n",
    "Why do we need to add the hook after the layernorm? The answer is that the residual stream can often grow in magnitude over time. Our rewards will be normalized (see later exercise), and so we want to make sure the outputs of our value head (which are estimates of the reward) also start off normalized.\n",
    "\"\"\"\n",
    "class TransformerWithValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a GPT model with a value head (the latter taking the last hidden state as input,\n",
    "    post-layernorm).\n",
    "\n",
    "    The value head is a simple MLP with one hidden layer, and scalar output:\n",
    "\n",
    "        Linear(d_model -> 4*d_model)\n",
    "        ReLU\n",
    "        Linear(4*d_model -> 1)\n",
    "\n",
    "    All linear layers have biases.\n",
    "    \"\"\"\n",
    "    base_model: HookedTransformer\n",
    "    value_head: nn.Sequential\n",
    "\n",
    "    def __init__(self, base_model: str = BASE_MODEL):\n",
    "        super().__init__()\n",
    "        self.base_model = HookedTransformer.from_pretrained(base_model)\n",
    "        d_model = self.base_model.cfg.d_model\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, 1)\n",
    "        )\n",
    "        self.value_head_output = None\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq\"]) -> tuple[\n",
    "        Float[Tensor, \"batch seq d_vocab\"],\n",
    "        Int[Tensor, \"batch seq\"]\n",
    "    ]:\n",
    "        def calc_and_store_value_head_output(resid_post, hook):\n",
    "            self.value_head_output = self.value_head(resid_post).squeeze(-1)\n",
    "        logits = self.base_model.run_with_hooks(\n",
    "            input_ids,\n",
    "            return_type=\"logits\",\n",
    "            fwd_hooks=[(utils.get_act_name(\"normalized\"), calc_and_store_value_head_output)]\n",
    "        )\n",
    "        return logits, self.value_head_output\n",
    "\n",
    "\n",
    "\n",
    "# Define a reference model (we'll use this during RLHF)\n",
    "model = TransformerWithValueHead().to(device)\n",
    "\n",
    "# Test your value head's architecture\n",
    "assert isinstance(model.base_model, HookedTransformer), \"Your model should have a HookedTransformer as its `base_model` attribute.\"\n",
    "assert isinstance(model.value_head, nn.Sequential), \"Your model should have a `value_head` attribute that is a `nn.Sequential`.\"\n",
    "d_model = model.base_model.cfg.d_model\n",
    "assert len(model.value_head) == 3, \"Your value head should be a `nn.Sequential` with 3 layers.\"\n",
    "assert sum(p.numel() for p in model.value_head.parameters()) == (d_model+1)*4*d_model + (4*d_model+1), \"Your value head should have the correct number of parameters.\"\n",
    "\n",
    "# Test your class's forward pass\n",
    "input_ids = t.randint(0, 1000, (1, 10)).to(device)\n",
    "logits, values = model(input_ids)\n",
    "assert logits.shape == (*input_ids.shape, model.base_model.cfg.d_vocab), \"Your model's logits should have shape (batch, seq, d_vocab).\"\n",
    "assert values.shape == input_ids.shape, \"Your model's value head should give you an output for every token in your input. Did you forget to squeeze the out_features=1 dim?\"\n",
    "\n",
    "print(\"All tests for `TransformerWithValueHead` passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.no_grad()\n",
    "def get_samples(base_model: HookedTransformer, prompt: str, batch_size: int, gen_len: int, temperature: float):\n",
    "    \"\"\"\n",
    "    Generates samples from the model, which will be fed into the reward model and evaluated.\n",
    "\n",
    "    Inputs:\n",
    "        gpt: the transformer to generate samples from (note we use gpt, not the model wrapper, cause we don't need value head)\n",
    "        prompt: the initial prompt fed into the model\n",
    "        batch_size: the number of samples to generate\n",
    "        gen_len: the length of the generated samples (i.e. the number of *new* tokens to generate)\n",
    "\n",
    "    Returns:\n",
    "        sample_ids: the token ids of the generated samples (including initial prompt)\n",
    "        samples: the generated samples (including initial prompt)\n",
    "    \"\"\"\n",
    "    # Make sure we've passed in the base model (the bit we use for sampling)\n",
    "    assert not isinstance(\n",
    "        base_model, TransformerWithValueHead\n",
    "    ), \"Please pass in the base model, not the model wrapper.\"\n",
    "\n",
    "    # Convert our prompt into tokens\n",
    "    input_ids = base_model.to_tokens(prompt, prepend_bos=False).squeeze(0)\n",
    "\n",
    "    # Generate samples (we repeat the input ids which is a bit wasteful but ¯\\_(ツ)_/¯)\n",
    "    input_ids = einops.repeat(input_ids, \"seq -> batch seq\", batch=batch_size)\n",
    "\n",
    "    # Generate samples\n",
    "    output_ids = base_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=gen_len,\n",
    "        stop_at_eos=False,\n",
    "        temperature=temperature,  # higher means more random completions\n",
    "        verbose=False,\n",
    "    )\n",
    "    samples = base_model.to_string(output_ids)\n",
    "\n",
    "    return output_ids.clone(), samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests for `reward_fn_char_count` passed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We'll start with a very basic reward function: counting the total number of periods in the sequence. For convenience, you should write your reward function to take in either a single sequence or a list of sequences (it will correspondingly return either a float or a tensor of floats).\n",
    "\n",
    "An interesting thing to note about this reward function - it counts over all characters, but the episode length is defined in terms of tokens. This means that theoretically our model could reward hack by outputting tokens with more than one . character. This particular model's vocabulary happens to include the token '.' * 64, so rewards would be through the roof if this was ever generated! However, remember that RL is about performing actions, getting feedback on those actions, and using that feedback to influence your policy. The token '.' * 64 is so unlikely to ever be generated that it'll probably never be positively reinforced, and we avoid this problem.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def reward_fn_char_count(generated_sample: str | list[str], char: str = '.') -> float | Float[Tensor, \"batch\"]:\n",
    "    \"\"\"\n",
    "    Reward function, evaluated on the generated samples.\n",
    "\n",
    "    In this case it's very simple: it just counts the number of instances of a particular character in\n",
    "    the generated sample. It returns a tensor of rewards of dtype float the input is a list, or a single\n",
    "    reward (float) if the input is a string.\n",
    "    \"\"\"\n",
    "    if isinstance(generated_sample, list):\n",
    "        return (\n",
    "            t.tensor([reward_fn_char_count(item) for item in generated_sample]).float()\n",
    "        )\n",
    "    else:\n",
    "        return float(generated_sample.count(char))\n",
    "\n",
    "\n",
    "# Test your reward function\n",
    "A = 'This is a test.'\n",
    "B = '......'\n",
    "C = 'Whatever'\n",
    "assert isinstance(reward_fn_char_count(A), float)\n",
    "assert reward_fn_char_count(A) == 1\n",
    "assert reward_fn_char_count(B) == 6\n",
    "assert reward_fn_char_count(C) == 0\n",
    "assert reward_fn_char_count([A, B, C]).dtype == t.float\n",
    "assert reward_fn_char_count([A, B, C]).tolist() == [1.0, 6.0, 0.0]\n",
    "\n",
    "print('All tests for `reward_fn_char_count` passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests for `normalize_reward` passed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Following advice from Ziegler el al. (2019), it's important to normalize the reward function over each batch (i.e. subtract mean and divide by std dev). We've been able to get away with not doing this so far because our reward functions were usually nicely bounded, e.g. the reward was always zero or one in cartpole (and even in our reward shaping it was still in the zero-one range). But if we're working with reward functions that could be much higher variance such as the number of periods in a generated sequence, then we should normalize.\n",
    "\n",
    "Note - we're not super strict about this function; the denominator being std + eps or (var + eps).sqrt() are both fine.\n",
    "\"\"\"\n",
    "\n",
    "def normalize_reward(reward: Float[Tensor, \"batch_size\"], eps=1e-5) -> Float[Tensor, \"batch_size\"]:\n",
    "    \"\"\"\n",
    "    Normalizes the reward function values over the batch of sequences.\n",
    "    \"\"\"\n",
    "    mean = reward.mean()\n",
    "    std_dev = reward.std() + eps\n",
    "    return (reward - mean) / std_dev\n",
    "\n",
    "\n",
    "# Test your reward normalization function\n",
    "reward = 10 + 5 * t.randn(10_000)\n",
    "reward_normalized = normalize_reward(reward)\n",
    "assert reward_normalized.mean().abs() < 1e-4\n",
    "assert (reward_normalized.std() - 1).abs() < 1e-4\n",
    "# Test edge case of zero reward\n",
    "reward = t.zeros(5)\n",
    "reward_normalized = normalize_reward(reward)\n",
    "assert reward_normalized.abs().sum() < 1e-4\n",
    "\n",
    "print('All tests for `normalize_reward` passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RLHFTrainingArgs():\n",
    "    # Basic / global\n",
    "    seed: int = 1\n",
    "    cuda: bool = t.cuda.is_available()\n",
    "\n",
    "    # Wandb / logging\n",
    "    exp_name: str = \"RLHF_Implementation\"\n",
    "    wandb_project_name: str | None = \"ch2-day4-rlhf\"\n",
    "    wandb_entity: str | None = None  \n",
    "    use_wandb: bool = False\n",
    "\n",
    "    # Duration of different phases\n",
    "    total_phases: int = 200\n",
    "    batch_size: int = 256\n",
    "    num_minibatches: int = 4\n",
    "    batches_per_learning_phase: int = 2\n",
    "\n",
    "    # Optimization hyperparameters\n",
    "    base_learning_rate: float = 2e-5\n",
    "    head_learning_rate: float = 5e-4\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 20\n",
    "    final_scale: float = 0.1\n",
    "\n",
    "    # Computing other PPO loss functions\n",
    "    clip_coef: float = 0.2\n",
    "    vf_coef: float = 0.15\n",
    "    ent_coef: float = 0.001\n",
    "\n",
    "    # Base model & sampling arguments\n",
    "    base_model: str = BASE_MODEL\n",
    "    gen_len: int = 30\n",
    "    temperature: float = 0.6\n",
    "    prefix: str = \"This is\"\n",
    "\n",
    "    # Extra stuff for RLHF\n",
    "    kl_coef: float = 1.0\n",
    "    reward_fn: Callable = reward_fn_char_count\n",
    "    normalize_reward: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert (\n",
    "            self.batch_size % self.num_minibatches == 0\n",
    "        ), \"Batch size should be divisible by the number of minibatches.\"\n",
    "        self.minibatch_size = self.batch_size // self.num_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_compute_advantages` passed!\n"
     ]
    }
   ],
   "source": [
    "@t.no_grad()\n",
    "def compute_advantages(\n",
    "    values: Float[Tensor, \"minibatch_size seq_len\"],\n",
    "    rewards: Float[Tensor, \"minibatch_size\"],\n",
    "    prefix_len: int,\n",
    ") -> Float[Tensor, \"minibatch_size gen_len\"]:\n",
    "    \"\"\"\n",
    "    Computes the advantages for the PPO loss function, i.e. A_pi(s, a) = Q_pi(s, a) - V_pi(s).\n",
    "\n",
    "    In this formula we replace Q(s, a) with the 1-step Q estimates, and V(s) with the 0-step value estimates.\n",
    "\n",
    "    Inputs:\n",
    "        values:\n",
    "            the value estimates for each token in the generated sequence\n",
    "        rewards:\n",
    "            the rewards for the entire generated sequence\n",
    "        prefix_len:\n",
    "            the length of the prefix (i.e. the length of the initial prompt)\n",
    "\n",
    "    Returns:\n",
    "        advantages:\n",
    "            the advantages for each token in the generated sequence (not the entire sequence)\n",
    "    \"\"\"\n",
    "    one_step_q_est = t.cat(\n",
    "        [\n",
    "            values[:, prefix_len:-1], #shape [minibatch_size, gen_len-1]\n",
    "            rewards[:, None], #shape [minibatch_size, 1]\n",
    "        ],\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    zero_step_value_est = values[:, prefix_len -1 : -1] # shape [minibatch_size, gen_len]\n",
    "    advantages = one_step_q_est - zero_step_value_est\n",
    "\n",
    "    return advantages\n",
    "\n",
    "# Test the function\n",
    "tests.test_compute_advantages(compute_advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplayMinibatch:\n",
    "    \"\"\"\n",
    "    Samples from the replay memory.\n",
    "    \"\"\"\n",
    "    sample_ids: Float[Tensor, \"minibatch_size seq_len\"]\n",
    "    logprobs: Float[Tensor, \"minibatch_size seq_len\"]\n",
    "    advantages: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    returns: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    ref_logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"]\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: RLHFTrainingArgs,\n",
    "        sample_ids: Float[Tensor, \"batch_size seq_len\"],\n",
    "        logprobs: Float[Tensor, \"batch_size seq_len\"],\n",
    "        advantages: Float[Tensor, \"batch_size gen_len\"],\n",
    "        values: Float[Tensor, \"batch_size seq_len\"],\n",
    "        ref_logits: Float[Tensor, \"batch_size seq_len d_vocab\"],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the replay memory, with all the data generated from the rollout phase at once.\n",
    "\n",
    "        The advantages are (batch_size, gen_len) because we only compute advantages for the generated\n",
    "        tokens. The other tensors are (batch_size, seq_len) because they are computed for all tokens.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.sample_ids = sample_ids\n",
    "        self.logprobs = logprobs\n",
    "        self.advantages = advantages\n",
    "        self.values = values\n",
    "        self.ref_logits = ref_logits\n",
    "\n",
    "\n",
    "    def get_minibatches(self):\n",
    "        \"\"\"\n",
    "        Generates a list of minibatches by randomly sampling from the replay memory. Each sequence appears\n",
    "        exactly `batches_per_learning_phase` times in total.\n",
    "        \"\"\"\n",
    "        minibatches = []\n",
    "\n",
    "        returns = self.advantages + self.values[:, -self.args.gen_len-1:-1]\n",
    "\n",
    "        for _ in range(self.args.batches_per_learning_phase):\n",
    "\n",
    "            idxs = t.randperm(self.args.batch_size).reshape(self.args.num_minibatches, self.args.minibatch_size)\n",
    "\n",
    "            for idx in idxs:\n",
    "                minibatches.append(\n",
    "                    ReplayMinibatch(\n",
    "                        sample_ids = self.sample_ids[idx],\n",
    "                        logprobs = self.logprobs[idx],\n",
    "                        advantages = self.advantages[idx],\n",
    "                        returns = returns[idx],\n",
    "                        ref_logits = self.ref_logits[idx],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error: your kl_div=0.4819464087486267, kl_div_correct=0.48865893483161926. Did you take the correct slice over sequence positions?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 37\u001b[0m\n\u001b[1;32m     31\u001b[0m     kl_div \u001b[38;5;241m=\u001b[39m kl_div[:, prefix_len:]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kl_coef \u001b[38;5;241m*\u001b[39m kl_div\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_calc_kl_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalc_kl_penalty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m tests\u001b[38;5;241m.\u001b[39mtest_calc_kl_penalty_stability(calc_kl_penalty)\n",
      "File \u001b[0;32m~/Projects/ARENA/ARENA_3.0/chapter2_rl/exercises/part4_rlhf/tests.py:33\u001b[0m, in \u001b[0;36mtest_calc_kl_penalty\u001b[0;34m(calc_kl_penalty)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: your \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_div\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_div_correct\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m. Did you slice over sequence positions?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(kl_div \u001b[38;5;241m-\u001b[39m kl_div_mistake2) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: your \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_div\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_div_correct\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m. Did you take the correct slice over sequence positions?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(kl_div \u001b[38;5;241m-\u001b[39m kl_div_mistake3) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: your \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_div\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkl_div_correct\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m. Did you use the KL div coefficient correctly?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Error: your kl_div=0.4819464087486267, kl_div_correct=0.48865893483161926. Did you take the correct slice over sequence positions?"
     ]
    }
   ],
   "source": [
    "def calc_kl_penalty(\n",
    "    logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"],\n",
    "    ref_logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"],\n",
    "    kl_coef: float,\n",
    "    prefix_len: int,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between the logits and the reference logits, scaled\n",
    "    by the penalty function. This is used to stop the learned policy from diverging\n",
    "    too much from the original reference model's policy.\n",
    "\n",
    "    logits:\n",
    "        The logits of the generated samples (under the new model).\n",
    "    ref_logits:\n",
    "        The logits of the generated samples (under the reference model).\n",
    "    kl_coef:\n",
    "        The coefficient of the KL penalty.\n",
    "    prefix_len:\n",
    "        The length of the prefix to ignore when computing the KL divergence.\n",
    "    \"\"\"\n",
    "    # convert the logits to probs using softmax\n",
    "    logprobs = logits.log_softmax(dim=-1)\n",
    "    ref_logprobs = ref_logits.log_softmax(dim=-1)\n",
    "\n",
    "    # compute the probabilities\n",
    "    probs = logprobs.exp()\n",
    "\n",
    "    # compute the KL divergence\n",
    "    kl_div = (probs * (logprobs - ref_logprobs)).sum(dim=-1)\n",
    "\n",
    "    kl_div = kl_div[:, prefix_len:]\n",
    "\n",
    "    return kl_coef * kl_div.mean()\n",
    "\n",
    "\n",
    "\n",
    "tests.test_calc_kl_penalty(calc_kl_penalty)\n",
    "tests.test_calc_kl_penalty_stability(calc_kl_penalty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
