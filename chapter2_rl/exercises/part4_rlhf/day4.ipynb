{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformer_lens in /home/bart/.local/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.34.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (1.4.3)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (4.44.2)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.13.10)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.14.1)\n",
      "Requirement already satisfied: einops>=0.6.0 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.6.0)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (3.0.1)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: sentencepiece in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.10 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (2.1.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (4.66.5)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.0.3)\n",
      "Requirement already satisfied: typing-extensions in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (4.12.2)\n",
      "Requirement already satisfied: rich>=12.6.0 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (13.8.0)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (0.2.34)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/bart/.local/lib/python3.10/site-packages (from transformer_lens) (2.1.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/bart/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.24.7)\n",
      "Requirement already satisfied: psutil in /home/bart/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bart/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/bart/.local/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable, Union\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "# !pip install eindex\n",
    "import eindex\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "!pip install transformer_lens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter2_rl\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part4_rlhf\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part4_rlhf.tests as tests\n",
    "import part4_rlhf.solutions as solutions\n",
    "\n",
    "device = t.device('mps' if t.backends.mps.is_available() else 'cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "LOW_GPU_MEM = False\n",
    "BASE_MODEL = \"gpt2-small\" if LOW_GPU_MEM else \"gpt2-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HookedTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mFirst define self.base_model and self.value_head in your init step (reminder that you should use HookedTransformer.from_pretrained to load in a pretrained model). Then rewrite the forward method so that it outputs both the logits from a forward pass and the output of the value head.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mWhy do we need to add the hook after the layernorm? The answer is that the residual stream can often grow in magnitude over time. Our rewards will be normalized (see later exercise), and so we want to make sure the outputs of our value head (which are estimates of the reward) also start off normalized.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerWithValueHead\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Defines a GPT model with a value head (the latter taking the last hidden state as input,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    post-layernorm).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    All linear layers have biases.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     base_model: HookedTransformer\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mTransformerWithValueHead\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerWithValueHead\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Defines a GPT model with a value head (the latter taking the last hidden state as input,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    post-layernorm).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    All linear layers have biases.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     base_model: \u001b[43mHookedTransformer\u001b[49m\n\u001b[1;32m     22\u001b[0m     value_head: nn\u001b[38;5;241m.\u001b[39mSequential\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m BASE_MODEL):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HookedTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "First define self.base_model and self.value_head in your init step (reminder that you should use HookedTransformer.from_pretrained to load in a pretrained model). Then rewrite the forward method so that it outputs both the logits from a forward pass and the output of the value head.\n",
    "\n",
    "The easiest and most direct way to get the output of the value head would be to add a hook to the residual stream before the unembedding matrix, which computes the output of the value head and stores it externally (or as a class attribute). You can review the material from section 1.2 if you don't remember how to use hooks, and you can refer to the diagram on the reference page for how to get the correct hook name.\n",
    "\n",
    "Why do we need to add the hook after the layernorm? The answer is that the residual stream can often grow in magnitude over time. Our rewards will be normalized (see later exercise), and so we want to make sure the outputs of our value head (which are estimates of the reward) also start off normalized.\n",
    "\"\"\"\n",
    "class TransformerWithValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a GPT model with a value head (the latter taking the last hidden state as input,\n",
    "    post-layernorm).\n",
    "\n",
    "    The value head is a simple MLP with one hidden layer, and scalar output:\n",
    "\n",
    "        Linear(d_model -> 4*d_model)\n",
    "        ReLU\n",
    "        Linear(4*d_model -> 1)\n",
    "\n",
    "    All linear layers have biases.\n",
    "    \"\"\"\n",
    "    base_model: HookedTransformer\n",
    "    value_head: nn.Sequential\n",
    "\n",
    "    def __init__(self, base_model: str = BASE_MODEL):\n",
    "        super().__init__()\n",
    "        self.base_model = HookedTransformer.from_pretrained(base_model)\n",
    "        d_model = self.base_model.cfg.d_model\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, 1)\n",
    "        )\n",
    "        self.value_head_output = None\n",
    "\n",
    "    def forward(self, input_ids: Int[Tensor, \"batch seq\"]) -> tuple[\n",
    "        Float[Tensor, \"batch seq d_vocab\"],\n",
    "        Int[Tensor, \"batch seq\"]\n",
    "    ]:\n",
    "        def calc_and_store_value_head_output(resid_post, hook):\n",
    "            self.value_head_output = self.value_head(resid_post).squeeze(-1)\n",
    "        logits = self.base_model.run_with_hooks(\n",
    "            input_ids,\n",
    "            return_type=\"logits\",\n",
    "            fwd_hooks=[(utils.get_act_name(\"normalized\"), calc_and_store_value_head_output)]\n",
    "        )\n",
    "        return logits, self.value_head_output\n",
    "\n",
    "\n",
    "\n",
    "# Define a reference model (we'll use this during RLHF)\n",
    "model = TransformerWithValueHead().to(device)\n",
    "\n",
    "# Test your value head's architecture\n",
    "assert isinstance(model.base_model, HookedTransformer), \"Your model should have a HookedTransformer as its `base_model` attribute.\"\n",
    "assert isinstance(model.value_head, nn.Sequential), \"Your model should have a `value_head` attribute that is a `nn.Sequential`.\"\n",
    "d_model = model.base_model.cfg.d_model\n",
    "assert len(model.value_head) == 3, \"Your value head should be a `nn.Sequential` with 3 layers.\"\n",
    "assert sum(p.numel() for p in model.value_head.parameters()) == (d_model+1)*4*d_model + (4*d_model+1), \"Your value head should have the correct number of parameters.\"\n",
    "\n",
    "# Test your class's forward pass\n",
    "input_ids = t.randint(0, 1000, (1, 10)).to(device)\n",
    "logits, values = model(input_ids)\n",
    "assert logits.shape == (*input_ids.shape, model.base_model.cfg.d_vocab), \"Your model's logits should have shape (batch, seq, d_vocab).\"\n",
    "assert values.shape == input_ids.shape, \"Your model's value head should give you an output for every token in your input. Did you forget to squeeze the out_features=1 dim?\"\n",
    "\n",
    "print(\"All tests for `TransformerWithValueHead` passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
