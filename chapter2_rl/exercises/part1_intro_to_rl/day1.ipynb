{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional, Union, Tuple\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install gym\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "\n",
    "Arr = np.ndarray\n",
    "max_episode_steps = 1000\n",
    "N_RUNS = 200\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = \"chapter2_rl\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = (exercises_dir / \"part1_intro_to_rl\").resolve()\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part1_intro_to_rl.utils as utils\n",
    "import part1_intro_to_rl.tests as tests\n",
    "from plotly_utils import imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class MultiArmedBandit(gym.Env):\n",
    "    '''\n",
    "    A class representing a multi-armed bandit environment, based on OpenAI Gym's Env class.\n",
    "\n",
    "    Attributes:\n",
    "        action_space (gym.spaces.Discrete): The space of possible actions, representing the arms of the bandit.\n",
    "        observation_space (gym.spaces.Discrete): The space of possible observations.\n",
    "        num_arms (int): The number of arms in the bandit.\n",
    "        stationary (bool): Indicates whether the reward distribution (i.e. the arm_reward_means) is stationary or not.\n",
    "        arm_reward_means (np.ndarray): The mean rewards for each arm.\n",
    "    '''\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "    num_arms: int\n",
    "    stationary: bool\n",
    "    arm_reward_means: np.ndarray\n",
    "\n",
    "    def __init__(self, num_arms=10, stationary=True):\n",
    "        '''\n",
    "        Initializes the MultiArmedBandit environment.\n",
    "\n",
    "        Args:\n",
    "            num_arms (int): The number of arms for the bandit. Defaults to 10.\n",
    "            stationary (bool): Whether the bandit has a stationary reward distribution. Defaults to True.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_arms = num_arms\n",
    "        self.stationary = stationary\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        self.action_space = gym.spaces.Discrete(num_arms)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, arm: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Takes an action by choosing an arm and returns the result of the action.\n",
    "\n",
    "        Args:\n",
    "            arm (ActType): The selected arm to pull in the bandit.\n",
    "\n",
    "        Returns:\n",
    "            tuple[ObsType, float, bool, dict]: A tuple containing the observation, reward, done flag, and additional info.\n",
    "        '''\n",
    "        assert self.action_space.contains(arm)\n",
    "        if not self.stationary:\n",
    "            q_drift = self.np_random.normal(loc=0.0, scale=0.01, size=self.num_arms)\n",
    "            self.arm_reward_means += q_drift\n",
    "            self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        reward = self.np_random.normal(loc=self.arm_reward_means[arm], scale=1.0)\n",
    "        obs = 0\n",
    "        done = False\n",
    "        info = dict(best_arm=self.best_arm)\n",
    "        return (obs, reward, done, info)\n",
    "\n",
    "    def reset(self, seed: int | None=None, options=None) -> ObsType:\n",
    "        '''\n",
    "        Resets the environment to its initial state.\n",
    "\n",
    "        Args:\n",
    "            seed (int | None): The seed for random number generation. Defaults to None.\n",
    "            return_info (bool): If True, return additional info. Defaults to False.\n",
    "            options (dict): Additional options for environment reset. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ObsType: The initial observation.\n",
    "        '''\n",
    "        super().reset(seed=seed)\n",
    "        if self.stationary:\n",
    "            self.arm_reward_means = self.np_random.normal(loc=0.0, scale=1.0, size=self.num_arms)\n",
    "        else:\n",
    "            self.arm_reward_means = np.zeros(shape=[self.num_arms])\n",
    "        self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        return 0\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        '''\n",
    "        Renders the state of the environment, in the form of a violin plot.\n",
    "        '''\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\"\n",
    "        bandit_samples = []\n",
    "        for arm in range(self.action_space.n):\n",
    "            bandit_samples += [np.random.normal(loc=self.arm_reward_means[arm], scale=1.0, size=1000)]\n",
    "        plt.violinplot(bandit_samples, showmeans=True)\n",
    "        plt.xlabel(\"Bandit Arm\")\n",
    "        plt.ylabel(\"Reward Distribution\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our env inside its wrappers looks like: <TimeLimit<OrderEnforcing<PassiveEnvChecker<MultiArmedBandit<ArmedBanditTestbed-v0>>>>>\n"
     ]
    }
   ],
   "source": [
    "gym.envs.registration.register(\n",
    "    id=\"ArmedBanditTestbed-v0\",\n",
    "    entry_point=MultiArmedBandit,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    nondeterministic=True,\n",
    "    reward_threshold=1.0,\n",
    "    kwargs={\"num_arms\": 10, \"stationary\": True},\n",
    ")\n",
    "\n",
    "env = gym.make(\"ArmedBanditTestbed-v0\")\n",
    "print(f\"Our env inside its wrappers looks like: {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    Base class for agents in a multi-armed bandit environment\n",
    "\n",
    "    (you do not need to add any implementation here)\n",
    "    '''\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, num_arms: int, seed: int):\n",
    "        self.num_arms = num_arms\n",
    "        self.reset(seed)\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def observe(self, action: ActType, reward: float, info: dict) -> None:\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed: int) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "def run_episode(env: gym.Env, agent: Agent, seed: int):\n",
    "    '''\n",
    "    Runs a single episode of interaction between an agent and an environment.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment in which the agent operates.\n",
    "        agent (Agent): The agent that takes actions in the environment.\n",
    "        seed (int): The seed for random number generation to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: A tuple containing arrays of rewards\n",
    "        received in each step and a flag indicating if the chosen arm was the best.\n",
    "    '''\n",
    "    (rewards, was_best) = ([], [])\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "    agent.reset(seed=seed)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        arm = agent.get_action()\n",
    "        (obs, reward, done, info) = env.step(arm)\n",
    "        agent.observe(arm, reward, info)\n",
    "        rewards.append(reward)\n",
    "        was_best.append(1 if arm == info[\"best_arm\"] else 0)\n",
    "\n",
    "    rewards = np.array(rewards, dtype=float)\n",
    "    was_best = np.array(was_best, dtype=int)\n",
    "    return (rewards, was_best)\n",
    "\n",
    "\n",
    "def run_agent(env: gym.Env, agent: Agent, n_runs=200, base_seed=1):\n",
    "    all_rewards = []\n",
    "    all_was_bests = []\n",
    "    base_rng = np.random.default_rng(base_seed)\n",
    "    for n in tqdm(range(n_runs)):\n",
    "        seed = base_rng.integers(low=0, high=10_000, size=1).item()\n",
    "        (rewards, corrects) = run_episode(env, agent, seed)\n",
    "        all_rewards.append(rewards)\n",
    "        all_was_bests.append(corrects)\n",
    "    return (np.array(all_rewards), np.array(all_was_bests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "\n",
    "    def get_action(self) -> ActType:\n",
    "        # TODO\n",
    "        # This agent should pick an arm at random, i.e. from the range [0, ..., num_arms-1], and return it as an integer.\n",
    "        # do that by using \n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"RandomAgent\"\n",
    "\n",
    "\n",
    "num_arms = 10\n",
    "stationary = True\n",
    "env = gym.make(\"ArmedBanditTestbed-v0\", num_arms=num_arms, stationary=stationary)\n",
    "agent = RandomAgent(num_arms, 0)\n",
    "all_rewards, all_corrects = run_agent(env, agent)\n",
    "\n",
    "print(f\"Expected correct freq: {1/10}, actual: {all_corrects.mean():.6f}\")\n",
    "assert np.isclose(all_corrects.mean(), 1/10, atol=0.05), \"Random agent is not random enough!\"\n",
    "\n",
    "print(f\"Expected average reward: 0.0, actual: {all_rewards.mean():.6f}\")\n",
    "assert np.isclose(all_rewards.mean(), 0, atol=0.05), \"Random agent should be getting mean arm reward, which is zero.\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Cheater Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement UCBA selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular RL & Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
