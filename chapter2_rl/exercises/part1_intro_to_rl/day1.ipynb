{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional, Union, Tuple\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install gym\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "\n",
    "Arr = np.ndarray\n",
    "max_episode_steps = 1000\n",
    "N_RUNS = 200\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = \"chapter2_rl\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = (exercises_dir / \"part1_intro_to_rl\").resolve()\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "import part1_intro_to_rl.utils as utils\n",
    "import part1_intro_to_rl.tests as tests\n",
    "from plotly_utils import imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = int\n",
    "ActType = int\n",
    "\n",
    "class MultiArmedBandit(gym.Env):\n",
    "    '''\n",
    "    A class representing a multi-armed bandit environment, based on OpenAI Gym's Env class.\n",
    "\n",
    "    Attributes:\n",
    "        action_space (gym.spaces.Discrete): The space of possible actions, representing the arms of the bandit.\n",
    "        observation_space (gym.spaces.Discrete): The space of possible observations.\n",
    "        num_arms (int): The number of arms in the bandit.\n",
    "        stationary (bool): Indicates whether the reward distribution (i.e. the arm_reward_means) is stationary or not.\n",
    "        arm_reward_means (np.ndarray): The mean rewards for each arm.\n",
    "    '''\n",
    "    action_space: gym.spaces.Discrete\n",
    "    observation_space: gym.spaces.Discrete\n",
    "    num_arms: int\n",
    "    stationary: bool\n",
    "    arm_reward_means: np.ndarray\n",
    "\n",
    "    def __init__(self, num_arms=10, stationary=True):\n",
    "        '''\n",
    "        Initializes the MultiArmedBandit environment.\n",
    "\n",
    "        Args:\n",
    "            num_arms (int): The number of arms for the bandit. Defaults to 10.\n",
    "            stationary (bool): Whether the bandit has a stationary reward distribution. Defaults to True.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_arms = num_arms\n",
    "        self.stationary = stationary\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        self.action_space = gym.spaces.Discrete(num_arms)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, arm: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        '''\n",
    "        Takes an action by choosing an arm and returns the result of the action.\n",
    "\n",
    "        Args:\n",
    "            arm (ActType): The selected arm to pull in the bandit.\n",
    "\n",
    "        Returns:\n",
    "            tuple[ObsType, float, bool, dict]: A tuple containing the observation, reward, done flag, and additional info.\n",
    "        '''\n",
    "        assert self.action_space.contains(arm)\n",
    "        if not self.stationary:\n",
    "            q_drift = self.np_random.normal(loc=0.0, scale=0.01, size=self.num_arms)\n",
    "            self.arm_reward_means += q_drift\n",
    "            self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        reward = self.np_random.normal(loc=self.arm_reward_means[arm], scale=1.0)\n",
    "        obs = 0\n",
    "        done = False\n",
    "        info = dict(best_arm=self.best_arm)\n",
    "        return (obs, reward, done, info)\n",
    "\n",
    "    def reset(self, seed: int | None=None, options=None) -> ObsType:\n",
    "        '''\n",
    "        Resets the environment to its initial state.\n",
    "\n",
    "        Args:\n",
    "            seed (int | None): The seed for random number generation. Defaults to None.\n",
    "            return_info (bool): If True, return additional info. Defaults to False.\n",
    "            options (dict): Additional options for environment reset. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ObsType: The initial observation.\n",
    "        '''\n",
    "        super().reset(seed=seed)\n",
    "        if self.stationary:\n",
    "            self.arm_reward_means = self.np_random.normal(loc=0.0, scale=1.0, size=self.num_arms)\n",
    "        else:\n",
    "            self.arm_reward_means = np.zeros(shape=[self.num_arms])\n",
    "        self.best_arm = int(np.argmax(self.arm_reward_means))\n",
    "        return 0\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        '''\n",
    "        Renders the state of the environment, in the form of a violin plot.\n",
    "        '''\n",
    "        assert mode == \"human\", f\"Mode {mode} not supported!\"\n",
    "        bandit_samples = []\n",
    "        for arm in range(self.action_space.n):\n",
    "            bandit_samples += [np.random.normal(loc=self.arm_reward_means[arm], scale=1.0, size=1000)]\n",
    "        plt.violinplot(bandit_samples, showmeans=True)\n",
    "        plt.xlabel(\"Bandit Arm\")\n",
    "        plt.ylabel(\"Reward Distribution\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement RandomAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Cheater Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement UCBA selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular RL & Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
